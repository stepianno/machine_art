{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This notebook is devoted to creating and training the autoencoder which will later be used to generate the art*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "nbpresent": {
     "id": "0fbb7a3c-3666-4ce1-8018-bbfc66ee4961"
    }
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Input\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, Dense\n",
    "from keras.models import Model, load_model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import shutil\n",
    "import os\n",
    "from os import listdir\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*My images begin as 224x224 pixel images with 3 layers for RGB. Through convolutions and maxpooling, they are condensed into an encoded tensor of dimensions 28x28x16. Hense an initial image of 150,528 features is shrunk to a size of 12,544 features, less than a tenth the original size, before then being decoded to recreate the original image of the same size.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "nbpresent": {
     "id": "75e8c2f8-3d6c-4722-873a-ab266fbf75f3"
    }
   },
   "outputs": [],
   "source": [
    "input_ = Input(shape=(224,224,3))\n",
    "\n",
    "x = Conv2D(512, (3,3), activation = 'relu', padding='same')(input_)\n",
    "x = MaxPooling2D((2,2), padding='same')(x)\n",
    "x = Conv2D(64, (3,3), activation = 'relu', padding='same')(x)\n",
    "x = MaxPooling2D((2,2), padding='same')(x)\n",
    "x = Conv2D(16, (3,3), activation = 'relu', padding='same')(x)\n",
    "encoded = MaxPooling2D((2,2), padding='same', name='encoded')(x)\n",
    "\n",
    "x = Conv2D(16, (3,3), activation = 'relu', padding='same')(encoded)\n",
    "x = UpSampling2D((2,2))(x)\n",
    "x = Conv2D(64, (3,3), activation = 'relu', padding='same')(x)\n",
    "x = UpSampling2D((2,2))(x)\n",
    "x = Conv2D(512, (3,3), activation = 'relu', padding='same')(x)\n",
    "x = UpSampling2D((2,2))(x)\n",
    "decoded = Conv2D(3, (3,3), activation = 'sigmoid', padding='same')(x)\n",
    "\n",
    "\n",
    "autoencoder = Model(input_, decoded)\n",
    "autoencoder.compile(optimizer='adam', loss='mse', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "nbpresent": {
     "id": "2193c5cc-05c3-4981-82a0-0785aa53b2d4"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 224, 224, 512)     14336     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 112, 112, 512)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 112, 112, 64)      294976    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 56, 56, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 56, 56, 16)        9232      \n",
      "_________________________________________________________________\n",
      "encoded (MaxPooling2D)       (None, 28, 28, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 28, 28, 16)        2320      \n",
      "_________________________________________________________________\n",
      "up_sampling2d_4 (UpSampling2 (None, 56, 56, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 56, 56, 64)        9280      \n",
      "_________________________________________________________________\n",
      "up_sampling2d_5 (UpSampling2 (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 112, 112, 512)     295424    \n",
      "_________________________________________________________________\n",
      "up_sampling2d_6 (UpSampling2 (None, 224, 224, 512)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 224, 224, 3)       13827     \n",
      "=================================================================\n",
      "Total params: 639,395\n",
      "Trainable params: 639,395\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*In order to create both a testing and validation directory, I move 2,500 images randomly from the original directory to a validation directory*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "nbpresent": {
     "id": "766e7f37-1edf-4ed0-848c-f9a65d66c084"
    }
   },
   "outputs": [],
   "source": [
    "images = listdir('../images/neural_image_bucket/ads/more_pinterest')\n",
    "valid_ind = np.random.choice(len(images), 2500, replace=False)\n",
    "for ind in valid_ind:\n",
    "    shutil.move('../images/neural_image_bucket/ads/more_pinterest/'+images[ind], \n",
    "                    '../images/neural_image_bucket/validation/images/'+images[ind])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*I then create paths to each directory and create generators to feed images to the autoencoder while training*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "nbpresent": {
     "id": "0dce4925-83f8-4bfd-b96f-a853bb94218d"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10087 images belonging to 2 classes.\n",
      "Found 2500 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "training_directory = os.path.join('../images/neural_image_bucket/ads/')\n",
    "validation_directory = os.path.join('../images/neural_image_bucket/validation/')\n",
    "\n",
    "datagen = ImageDataGenerator(rescale=1./255) #this normalizes the input as all pixel strength is out of 255\n",
    "train = datagen.flow_from_directory(\n",
    "                training_directory,\n",
    "                target_size=(224, 224), #input size for the generator\n",
    "                batch_size=20, #limit the batch size so as not to overwork the computer\n",
    "                class_mode='input') #the final output should be the same as the input\n",
    "\n",
    "validate = datagen.flow_from_directory(\n",
    "                validation_directory,\n",
    "                target_size=(224, 224),\n",
    "                batch_size=20,\n",
    "                class_mode='input')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "nbpresent": {
     "id": "bbdbfc0f-db4f-4099-b621-60b1bb718386"
    }
   },
   "outputs": [],
   "source": [
    "#Model Checkpoint automatically saves the best performing model to date for optimization in training\n",
    "checkpoint = ModelCheckpoint('../auto_saving.h5', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "nbpresent": {
     "id": "fcea6f4b-0b1a-4ea7-bfdb-558f1f8978e1"
    }
   },
   "outputs": [],
   "source": [
    "training_samples = train.samples\n",
    "training_steps = training_samples / 20\n",
    "#setting up the batches\n",
    "validation_samples = validate.samples\n",
    "validation_steps = validation_samples / 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "nbpresent": {
     "id": "0e74cc87-584a-4ebe-b74a-e3777d8b5e93"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "505/504 [==============================] - 730s 1s/step - loss: 0.0103 - acc: 0.7695 - val_loss: 0.0123 - val_acc: 0.7108\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.01233, saving model to /home/stepianno/auto_saving.h5\n",
      "Epoch 2/15\n",
      "505/504 [==============================] - 729s 1s/step - loss: 0.0100 - acc: 0.7732 - val_loss: 0.0103 - val_acc: 0.8029\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.01233 to 0.01025, saving model to /home/stepianno/auto_saving.h5\n",
      "Epoch 3/15\n",
      "505/504 [==============================] - 727s 1s/step - loss: 0.0099 - acc: 0.7678 - val_loss: 0.0102 - val_acc: 0.8045\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.01025 to 0.01018, saving model to /home/stepianno/auto_saving.h5\n",
      "Epoch 4/15\n",
      "505/504 [==============================] - 728s 1s/step - loss: 0.0096 - acc: 0.7765 - val_loss: 0.0068 - val_acc: 0.8069\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.01018 to 0.00678, saving model to /home/stepianno/auto_saving.h5\n",
      "Epoch 5/15\n",
      "505/504 [==============================] - 728s 1s/step - loss: 0.0094 - acc: 0.7779 - val_loss: 0.0098 - val_acc: 0.7236\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.00678\n",
      "Epoch 6/15\n",
      "505/504 [==============================] - 725s 1s/step - loss: 0.0092 - acc: 0.7792 - val_loss: 0.0091 - val_acc: 0.7361\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.00678\n",
      "Epoch 7/15\n",
      "505/504 [==============================] - 725s 1s/step - loss: 0.0092 - acc: 0.7785 - val_loss: 0.0076 - val_acc: 0.8198\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.00678\n",
      "Epoch 8/15\n",
      "505/504 [==============================] - 725s 1s/step - loss: 0.0090 - acc: 0.7809 - val_loss: 0.0096 - val_acc: 0.7316\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.00678\n",
      "Epoch 9/15\n",
      "505/504 [==============================] - 724s 1s/step - loss: 0.0089 - acc: 0.7783 - val_loss: 0.0091 - val_acc: 0.7609\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.00678\n",
      "Epoch 10/15\n",
      "505/504 [==============================] - 724s 1s/step - loss: 0.0088 - acc: 0.7769 - val_loss: 0.0083 - val_acc: 0.7774\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.00678\n",
      "Epoch 11/15\n",
      "505/504 [==============================] - 725s 1s/step - loss: 0.0086 - acc: 0.7802 - val_loss: 0.0066 - val_acc: 0.7786\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.00678 to 0.00660, saving model to /home/stepianno/auto_saving.h5\n",
      "Epoch 12/15\n",
      "505/504 [==============================] - 724s 1s/step - loss: 0.0085 - acc: 0.7815 - val_loss: 0.0088 - val_acc: 0.8088\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.00660\n",
      "Epoch 13/15\n",
      "505/504 [==============================] - 724s 1s/step - loss: 0.0085 - acc: 0.7776 - val_loss: 0.0075 - val_acc: 0.7559\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.00660\n",
      "Epoch 14/15\n",
      "505/504 [==============================] - 724s 1s/step - loss: 0.0084 - acc: 0.7813 - val_loss: 0.0068 - val_acc: 0.7968\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.00660\n",
      "Epoch 15/15\n",
      "505/504 [==============================] - 723s 1s/step - loss: 0.0084 - acc: 0.7769 - val_loss: 0.0092 - val_acc: 0.8086\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.00660\n"
     ]
    }
   ],
   "source": [
    "#Finally, training the autoencoder! This takes about two hours on the cloud with a gpu.\n",
    "history = autoencoder.fit(\n",
    "      train,\n",
    "      steps_per_epoch=training_steps,\n",
    "      epochs=15,\n",
    "      verbose = True,\n",
    "      validation_data=validate,\n",
    "      validation_steps=validation_steps,\n",
    "      callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "nbpresent": {
     "id": "40384eb7-99c9-4010-b292-211476f291f7"
    }
   },
   "outputs": [],
   "source": [
    "autoencoder = load_model('../auto_saving.h5') #in order to load the saved autoencoder when returning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*In order to make new images with the autoencoder, I need to aplit it in half and save both the encoder and decoder separately. The encoder is a bit easier to save as it has the same input shape as the autoencoder. For the decoder, I must reconstruct the second half of the autoencoder and load the weights by name.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = Model(inputs=autoencoder.input, outputs=autoencoder.get_layer('encoded').output)\n",
    "feature_extractor.save('../feature.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ = Input(shape=(28,28,16))\n",
    "\n",
    "x = Conv2D(16, (3,3), activation = 'relu', padding='same', name='conv2d_11')(input_)\n",
    "x = UpSampling2D((2,2), name='up_sampling2d_4')(x)\n",
    "x = Conv2D(64, (3,3), activation = 'relu', padding='same', name='conv2d_12')(x)\n",
    "x = UpSampling2D((2,2), name='up_sampling2d_5')(x)\n",
    "x = Conv2D(512, (3,3), activation = 'relu', padding='same', name='conv2d_13')(x)\n",
    "x = UpSampling2D((2,2), name='up_sampling2d_6')(x)\n",
    "decoded = Conv2D(3, (3,3), activation = 'sigmoid', padding='same', name='conv2d_14')(x)\n",
    "\n",
    "image_maker = Model(input_, decoded)\n",
    "image_maker.load_weights('../auto_saving.h5', by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_maker.save('/home/stepianno/imager.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
