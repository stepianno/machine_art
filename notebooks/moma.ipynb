{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This notebook is dedicated to scraping all images from MoMA. For this project I use the 2,000+ collection of paintings in order to generate my own modern art*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from fake_useragent import UserAgent\n",
    "import time, os\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "chromedriver = \"/Applications/chromedriver\" # path to the chromedriver executable\n",
    "os.environ[\"webdriver.chrome.driver\"] = chromedriver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The url here, is a link to all modern art paintings in the online collection. When clicking the 'Show more results' button, the url actually changes and will no longer include html for the first 40 images, so we have to do the first 40 separately*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.moma.org/collection/?utf8=%E2%9C%93&q=&classifications=9&date_begin=Pre-1850&date_end=2020'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = soup.find(class_='grid') # grid contains all the images\n",
    "lis = grid.find_all('li') # all images are stored in a li within the grid, along with their title and artist\n",
    "for li in lis:\n",
    "    title = []\n",
    "    for span in li.find('h3').find_all('span'): #the h3 contains artist, title, year, all in separate spans\n",
    "        title.append(span.text) #the three components of the name are compressed here into a list\n",
    "    name = '_'.join([re.sub(' ', '-', bit.strip()) for bit in title])+'.jpeg' #I join the list into a string and make it a jpeg\n",
    "    name = re.sub('/', '-', name) #any slashes will ruin the path and not allow it to save correctly\n",
    "    try:\n",
    "        href = li.find('picture').find('img')['src']\n",
    "        response = requests.get('https://www.moma.org'+href)\n",
    "        file = open(f\"moma_plus/{name}\", \"wb\")\n",
    "        file.write(response.content)\n",
    "        file.close()\n",
    "    except:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The above code only obtains the initial 40 images. To get the other 2,000+, I use the code below. I have to use Selenium in order to load all the images onto the page.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.moma.org/collection/?utf8=%E2%9C%93&q=&classifications=9&date_begin=Pre-1850&date_end=2020&page=2&direction=fwd'\n",
    "driver = webdriver.Chrome(chromedriver)\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "while True:\n",
    "    driver.find_element_by_xpath('//*[contains(text(), \"Show more results\")]').click() \n",
    "    #Everytime the button is clicked, another 40 or so images is loaded\n",
    "    time.sleep(8) #The 8 seconds of sleep is necessary to allow the page to load before trying to click again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Once all images are loaded, I save the page to BeautifulSoup and follow the same process as before*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(driver.page_source, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = soup.find(class_='grid') # grid contains all the images\n",
    "lis = grid.find_all('li') # all images are stored in a li within the grid, along with their title and artist\n",
    "for li in lis:\n",
    "    title = []\n",
    "    for span in li.find('h3').find_all('span'): #the h3 contains artist, title, year, all in separate spans\n",
    "        title.append(span.text) #the three components of the name are compressed here into a list\n",
    "    name = '_'.join([re.sub(' ', '-', bit.strip()) for bit in title])+'.jpeg' #I join the list into a string and make it a jpeg\n",
    "    name = re.sub('/', '-', name) #any slashes will ruin the path and not allow it to save correctly\n",
    "    try:\n",
    "        href = li.find('picture').find('img')['src']\n",
    "        response = requests.get('https://www.moma.org'+href)\n",
    "        file = open(f\"moma_plus/{name}\", \"wb\") #The image is saved according to its official name\n",
    "        file.write(response.content)\n",
    "        file.close()\n",
    "    except:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
